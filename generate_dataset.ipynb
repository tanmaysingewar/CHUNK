{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index llama-index-embeddings-huggingface llama-index-llms-openai load_dotenv spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "url = \"https://api.perplexity.ai/chat/completions\"\n",
    "\n",
    "DEFAULT_CONTEXT_TEMPLATE = (\n",
    "    \"Context information is below.\"\n",
    "    \"\\n--------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n--------------------\\n\"\n",
    ")\n",
    "\n",
    "def chat_completion(context_str,model,question):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_CONTEXT_TEMPLATE.format(context_str=context_str)\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": f\"Bearer {os.getenv('PERPLEXITY_API_KEY')}\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        json_response = response.json()\n",
    "        return json_response.get(\"choices\")[0].get(\"message\").get(\"content\")\n",
    "    elif response.status_code == 429:\n",
    "        return response.status_code\n",
    "    elif response.status_code == 400:\n",
    "        return response.status_code\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat form here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import signal\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "top_k_list = [3, 12, 20, 25, 30, 35, 38, 40]\n",
    "\n",
    "\n",
    "def response_generator(model_name,question_file,index_dir):\n",
    "    timeout_duration = 10\n",
    "\n",
    "    vector_index = load_index_from_storage(storage_context=StorageContext.from_defaults(persist_dir=rf'index/{index_dir}'),embed_model=embed_model,llm=None)\n",
    "    # query_engine = index.as_query_engine()\n",
    "\n",
    "    with open(question_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        csv_data = list(reader)\n",
    "\n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException(\"Operation timed out\")\n",
    "\n",
    "    # Loop from 3 to 30 top_k\n",
    "    try:\n",
    "        for top_k in top_k_list:\n",
    "            print(f\" ----------------- Details ------------------ \")\n",
    "            print(f\"Model Name: {model_name}\")\n",
    "            print(f\"Top K: {top_k}\")\n",
    "            print(f\"Number of Questions: {len(csv_data) - 1}\")\n",
    "            print(f\"Index: index/{index_dir}\")\n",
    "            print(f\"--------------------------------------------- \")\n",
    "\n",
    "            for row in csv_data:\n",
    "                index = csv_data.index(row)\n",
    "                if index == 0:\n",
    "                    csv_data[index].append(f\"{model_name} top_k={top_k}\")\n",
    "                    csv_data[index].append(f\"Response time of {model_name} top_k={top_k}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"-------------------------- Question No : {index} & Top K : {top_k} --------------------------\")\n",
    "                print(f\"Question: {row[0]}\")\n",
    "                \n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(timeout_duration)\n",
    "\n",
    "                try:\n",
    "                    print(f\"Case does not exist\")\n",
    "                    retriever = vector_index.as_retriever(similarity_top_k=top_k)\n",
    "                    retrieved_nodes = retriever.retrieve(str(row[0]))\n",
    "                    context_str = \"\\n\\n\".join([n.node.get_content(metadata_mode=MetadataMode.LLM).strip() for n in retrieved_nodes])\n",
    "                    print(f\"Vector Response: {context_str}\")\n",
    "                    signal.alarm(0)\n",
    "                except TimeoutException:\n",
    "                    print(f\"Error: Unable to retrieve vector response (Timeout)\")\n",
    "                    with open(f'csv/{index_dir}-index-response.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        csv_data[index].append(\"Error : Unable to retrieve vector response (Timeout)\")\n",
    "                        csv_data[index].append(f\"Error : Unable to retrieve vector response (Timeout)\")\n",
    "                        writer.writerows(csv_data)\n",
    "\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    with open(f'csv/{index_dir}-index-response.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        csv_data[index].append(\"Error : Unable to retrieve vector response\")\n",
    "                        csv_data[index].append(f\"Error : Unable to retrieve vector response\")\n",
    "                        writer.writerows(csv_data)\n",
    "\n",
    "                    continue\n",
    "\n",
    "\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(timeout_duration)\n",
    "\n",
    "                try:\n",
    "                    context_str = f\"Context: {context_str}\"\n",
    "                    question_text = f\"Question: {row[0]}\\n\"\n",
    "                    prompt = f\"Using the provided context, answer the following question: \\n{question_text}\"\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    response_llm = chat_completion(model=model_name, question=question_text, context_str=context_str)\n",
    "                    end_time = time.time()\n",
    "                    response_time = end_time - start_time\n",
    "                    print(f\"LLM Response: {response_llm}\")\n",
    "                    print(f\"Response Time: {response_time:.4f} seconds\")\n",
    "\n",
    "                    # Append the LLM response and the corresponding response time\n",
    "                    csv_data[index].append(response_llm)\n",
    "                    csv_data[index].append(f\"{response_time:.4f} seconds\")\n",
    "\n",
    "                    signal.alarm(0)\n",
    "                    with open(f'csv/{index_dir}-index-response.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        writer.writerows(csv_data)\n",
    "\n",
    "                except TimeoutException:\n",
    "                    print(f\"Error: Unable to retrieve LLM response (Timeout)\")\n",
    "                    with open(f'csv/{index_dir}-index-response.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        csv_data[index].append(\"Error : Unable to retrieve LLM response (Timeout)\")\n",
    "                        csv_data[index].append(f\"Error : Unable to retrieve LLM response (Timeout)\")\n",
    "                        writer.writerows(csv_data)\n",
    "\n",
    "                    continue\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    with open(f'csv/{index_dir}-index-response.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "                        writer = csv.writer(file)\n",
    "                        csv_data[index].append(\"Error : Unable to retrieve LLM response\")\n",
    "                        csv_data[index].append(f\"Error : Unable to retrieve LLM response\")\n",
    "                        writer.writerows(csv_data)\n",
    "                        \n",
    "                    continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'llama-3.1-8b-instruct' \n",
    "question_file = 'csv/legal-questions.csv'\n",
    "index_dir = 'legal/512-legal' \n",
    "\n",
    "response_generator(model_name,question_file,index_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
